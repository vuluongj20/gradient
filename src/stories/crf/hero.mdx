import HeroImage from './heroImage'

import '@layouts/katex.css'

<Header>

# Naming Names with Graphical Models

<Abstract>
  Understanding how model structure and statistical assumptions can affect predictions in
  a named-entity recognition task.
</Abstract>

</Header>

<HeroImage />

Consider the following sentences:

> "A light _bass_ provided much-needed balance to the song."
>
> "A silver sea _bass_ flashed by the boat."

_"Bass"_ means something different (and is pronounced in a different way) in each sentence. A human reader would find the meanings to be rather intuitive given the available context. For example, the adjectives "silver" and "sea" that precede "bass" in the second sentence tells us that the subject is something aquatic. Since it "flashed" by a boat, it must be a fish, hence "bass" must mean the type of fish.

This chain of reasoning happens quickly and comes as second nature to human readers. But can we teach a machine to reason in a similar way? Can a language model disambiguate the role and meaning of words that are highly dependent on those around them?

Without resorting to complex neural networks, there are compact probabilistic models provide reliable and interpretable predictions in many sequence modeling problems, like part-of-speech tagging and named entity recognition.

In this article, we’ll explore three such models, each building on top of the one before: Hidden Markov Models (HMMs), Maximum-Entropy Markov Models (MEMMs), and Conditional Random Fields (CRFs). All three are probabilistic graphical models, which we’ll cover in the next section.

---
