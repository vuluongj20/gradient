import HeroImage from './heroImage'

import '@layouts/katex.css'

<Header>

# Linear-Chain Probabilistic Graphical Models

<Abstract>
  Probabilistic graphical models like Conditional Random Fields are powerful frameworks
  for solving sequence modeling problems.
</Abstract>

</Header>

<HeroImage />

Natural language is sequential but highly contextual, comprising of multiple interdependent parts. Consider the following sentences:

> “I can never sing _bass_.”
>
> "A light _bass_ provided balance to the song."
>
> "A large, beautiful _bass_ flashed by the boat."

_"Bass"_ means something different in each sentence and sounds different in the third sentence compared to the first two. A human reader would find the meanings to be rather intuitive given the context available in each sentence. For example, the fact that the "bass" flashed by a boat tells us that it can move and is likely underwater. Given the limited set of definitions for "bass", we quickly infer that it must be referring to a type of fish.

This chain of reasoning happens quickly and comes as second nature to human readers. But can we teach a machine to reason in a similar way? Can a language model disambiguate the role and meaning of words that are highly dependent on those around them?

Without resorting to complex neural networks, there are compact probabilistic models provide reliable and interpretable predictions in many sequence modeling problems, like part-of-speech tagging and named entity recognition.

In this article, we’ll explore three such models, each building on top of the one before: Hidden Markov Models (HMMs), Maximum-Entropy Markov Models (MEMMs), and Conditional Random Fields (CRFs). All three are probabilistic graphical models, which we’ll cover in the next section.

---
