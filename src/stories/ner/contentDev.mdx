import { Fragment } from 'react'

import LivePrediction from './livePrediction'
import ABCDGraph from './partI/sampling/abcdGraph'
import StaticABCDGraph from './partI/staticABCDGraph'
import HMMBinaryOOVHeatmap from './partII/binaryOOVHeatmap'
import HMMEmissionGraph from './partII/emissionGraph'
import HMMEmissionPaths from './partII/emissionPaths'
import HMMEntityLengthHeatmap from './partII/entityLengthHeatmap'
import HMMNERGraph from './partII/nerGraph'
import HMMOOVRateHeatmap from './partII/oovRateHeatmap'
import HMMOverviewGraph from './partII/overviewGraph'
import HMMPerEntityScorecards from './partII/perEntityScorecards'
import HMMPerWordScorecards from './partII/perWordScorecards'
import HMMTransitionGraph from './partII/transitionGraph'
import HMMTransitionPaths from './partII/transitionPaths'
import MEMMDiscriminativeGraph from './partIII/discriminativeGraph'
import MEMMFeatureBreakdown from './partIII/featureBreakdown'
import MEMMOverviewGraph from './partIII/overviewGraph'

<Header>

# Learning What’s in a Name with Graphical Models

{

<Abstract>
	An overview of the structure and statistical assumptions behind linear-chain graphical
	models – effective and interpretable approaches to sequence labeling problems such as
	named entity recognition.
</Abstract>
}

<LivePrediction
	hideTagPrefixes
	models={['hmm', 'memm', 'crf']}
	initialInputValue="East Timorese-born activist Jose Ramos Horta"
	label={
		<Fragment>
			Which of these words refer to a&nbsp;
			<abbr title="A person, organization, location, or other (miscellaneous)">
				<span>named entity</span>
			</abbr>?
		</Fragment>
	}
/>

</Header>

---

## III. Maximum Entropy Markov Models

Maximum Entropy Markov Models (MEMMs) resolve some of the concerns with HMMs by way of a simple yet meaningful modification in graphical structure:

<MEMMOverviewGraph />

Similar to HMMs, MEMMs have been successfully applied to a wide range of sequence modeling problems [@McCallum2000; @Thierry2010; @Ziebart2009].

### Discriminative Structure

Suppose there is a system with two variables, $X$ and $Y$, and we want to make predictions on $Y$ based on observations on $X$. A generative classifier would do that by first learning the prior distribution $p(X, Y)$ and then applying Bayes' rule to find the posterior $p(Y|X)$. A discriminative classifier, on the other hand, would model the posterior $p(Y|X)$ directly. [@Ng2001]

HMMs are generative generative classifiers, while MEMMs are discriminative. HMMs are generative because they model the joint distribution over both observations and hidden states, before using that to identify the most likely state sequence given observations or solve some other inference problems. MEMMs, on the other hand, directly model the conditional distribution $p(states|observations)$.

<MEMMDiscriminativeGraph />

### Word Features

The biggest advatage that MEMMs have over HMMs is their ability to model overlapping word features. Two features can overlap when they contain the same or similar pieces of information, like word shape (“Xxx”) and capitalization. HMMs don’t allow overlapping features, since as a result of their generative structure they require that all events in the observation layer be independent of one another. MEMMs, on the other hand, are discriminative and not subject to the independence requirement, so they can use any number of word features, overlapping or not. [@McCallum2000]

Common practice is to use binary features, such as:

$$
b(o_t) = \begin{cases}
	1\ \textrm{ if $o_t$ has word shape "Xxx"} \\
	0\ \textrm{ otherwise}
	\end{cases}
$$

These features are then paired with the current state $s$ to form feature-state pairs $a = \langle b, s \rangle$:

$$
f_a(o_t, s_t) = f_{\langle b, s \rangle}(o_t, s_t) = \begin{cases}
	1\ \textrm{ if $b(o_t) = 1$ and $s_t = s$} \\
	0\ \textrm{ otherwise}
	\end{cases}
$$

Feature-state pairs provide useful information about which features and states go together and which don’t. For example, we expect to see fewer occurrences of the "capitalized" + “O” pair than say "capitalized" + “B-ORG”.

A weighted sum of the values of all feature-state pairs is the main parameter for the state transition distribution, which has exponential form:

$$
P_{s\prime}(s|o) = \frac{1}{Z(o, s\prime)} \exp\left(\sum_{a} \lambda_a \ f_a(o, s)\right)
$$

where $s\prime$ and $s$ are the previous and current state, $o$ is the current observation, $a$ is a feature-state pair, $\lambda_a$ is the learned weight for $a$, and $Z(o, s\prime)$ is a normalizing term to make the distribution $P_{s\prime}$ sum to one across all next states $s$. [@McCallum2000]

<MEMMFeatureBreakdown />

---

<References />
