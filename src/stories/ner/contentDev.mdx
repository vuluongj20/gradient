import { Fragment } from 'react'

import CRFOverviewGraph from './partIV/overviewGraph'
import CRFRandomFieldGraph from './partIV/randomFieldGraph'
import CRFUndirectedGraphExample from './partIV/undirectedGraphExample'

<Header></Header>

## IV. Conditional Random Fields

Rather than assuming a generative relationship between variables, MRFs model their mutual relationships with non-negative scoring functions, called _factors_, that assign higher scores if the variables’ values are in agreement, for example:

$$
f(X, Y) = \begin{cases}
	\textrm{10 if $X = 1$ and $Y = 1$} \\
	\textrm{5 if $X = 0$ and $Y = 0$} \\
	\textrm{1 otherwise}
	\end{cases}
$$

Unlike conditional probabilities, there is no assumed directionality in scoring functions. These functions simply return higher scores if the variables agree and lower scores if they disagree. They model pairwise correlation, not causation.

The joint probability of all variables in the graph is:

$$
p(A,B,C) = \frac{1}{Z}\,f(A,B)\,f(B,C)\,f(C,A) \\
\footnotesize\textrm{where Z is a normalization factor}
$$

The factors $f$ promote assignments in which their constituent variables ($A$ and $B$ in the case of $f(A, B)$) agree with each other. The assignment 1-1-1 would receive a higher score and thus have higher probability than say 1-1-0, since there is more agreement in the former case.

More generally, MRFs are probability distributions $p$ over random variables $x_1$, $x_2$,… that are defined by an undirected graph $\mathcal{G}$ and have the form:

$$
p(x_1, x_2,…) = \frac{1}{Z}\prod_{c\,{\scriptscriptstyle \in}\,C}{f_c(x_c)} \\
\footnotesize\textrm{where Z is a normalization factor} \\
\footnotesize\textrm{and C is the set of maximal cliques in $\mathcal{G}$}
$$

<CRFUndirectedGraphExample />

MRFs have a generalized form of which the directed models we’ve seen so far — HMMs and MEMMs — are special cases. The factors $f_c$ can be defined as conditional probabilities, for example $f_c(x_1,x_2) = p(x_2|x_1)$, and act as the transition and emission probabilities that together compose HMMs and MEMMs.

The additional level of generality comes at a cost: the normalization factors $Z$ are often difficult to compute. They require summing over an exponential number of potential assignments, an infeasible task if the network is large enough. There are, however, configurations for which there is an efficient decoding algorithm. That includes linear-chain versions of CRFs, which can be decoded with the Viterbi algorithm.

---

<References />
