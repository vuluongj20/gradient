import { Fragment } from 'react'

import LivePrediction from './livePrediction'
import ABCDGraph from './partI/sampling/abcdGraph'
import StaticABCDGraph from './partI/staticABCDGraph'
import HMMEmissionGraph from './partII/emissionGraph'
import HMMEmissionPaths from './partII/emissionPaths'
import HMMNERGraph from './partII/nerGraph'
import HMMOverviewGraph from './partII/overviewGraph'
import HMMPerEntityScorecards from './partII/perEntityScorecards'
import HMMPerWordScorecard from './partII/perWordScorecard'
import HMMTransitionGraph from './partII/transitionGraph'
import HMMTransitionPaths from './partII/transitionPaths'
import MEMMDiscriminativeGraph from './partIII/discriminativeGraph'
import MEMMFeatureBreakdown from './partIII/featureBreakdown'
import MEMMOverviewGraph from './partIII/overviewGraph'
import MEMMPerEntityScorecards from './partIII/perEntityScorecards'
import MEMMPerWordScorecard from './partIII/perWordScorecards'
import BinaryOOVHeatmap from './precisionRecallHeatmaps/binaryOOVHeatmap'
import EntityLengthHeatmap from './precisionRecallHeatmaps/entityLengthHeatmap'
import OOVRateHeatmap from './precisionRecallHeatmaps/oovRateHeatmap'

<Header>

# Learning What’s in a Name with Graphical Models

{

<Abstract>
	An overview of the structure and statistical assumptions behind linear-chain graphical
	models – effective and interpretable approaches to sequence labeling problems such as
	named entity recognition.
</Abstract>
}

<LivePrediction
	hideTagPrefixes
	models={['hmm', 'memm', 'crf']}
	initialInputValue="East Timorese-born activist Jose Ramos Horta"
	label={
		<Fragment>
			Which of these words refer to a&nbsp;
			<abbr title="A person, organization, location, or other (miscellaneous)">
				<span>named entity</span>
			</abbr>?
		</Fragment>
	}
/>

</Header>

---

## III. Maximum Entropy Markov Models

Maximum Entropy Markov Models (MEMMs) resolve some of the concerns with HMMs by way of a simple yet meaningful modification in graphical structure:

<MEMMOverviewGraph />

Similar to HMMs, MEMMs have been successfully applied to a wide range of sequence modeling problems [@McCallum2000; @Thierry2010; @Ziebart2009].

### Discriminative Structure

Suppose there is a system with two variables, $X$ and $Y$, and we want to make predictions about $Y$ based on observations of $X$. A generative classifier would do that by first learning the prior distribution $p(X, Y)$ and then applying Bayes' rule to find the posterior $p(Y|X)$. In a way, it attempts to reconstruct the process that generated the observed data. A discriminative classifier, on the other hand, would model the posterior $p(Y|X)$ directly based on training data. [@Ng2001]

While HMMs are generative classifiers, MEMMs are discriminative. The former are generative because they model the joint distribution over both observations and hidden states (as a product of transition and emission probabilities) before using that joint distribution to find the most likely state sequence given observations (or solve some other inference problem). MEMMs, on the other hand, directly model the conditional distribution $p(states|observations)$ without any intermediaries.

<MEMMDiscriminativeGraph />

### Word Features

The biggest advatage that MEMMs have over HMMs is their ability to model overlapping word features. Two features can overlap when they contain the same or similar pieces of information, like word shape (“Xxx”) and capitalization. HMMs don’t allow overlapping features, since as a result of their generative structure they require that all events in the observation layer be independent of one another. MEMMs, on the other hand, are discriminative and not subject to the independence requirement, so they can use any number of word features, overlapping or not. [@McCallum2000]

Common practice is to use binary features, such as:

$$
b(o_t) = \begin{cases}
	1\ \textrm{ if $o_t$ has shape "Xxx"} \\
	0\ \textrm{ otherwise}
	\end{cases}
$$

These features are then paired with the current state $s$ to form feature-state pairs $a = \langle b, s \rangle$:

$$
f_{\langle b, s \rangle}(o_t, s_t) = \begin{cases}
	1\ \textrm{ if $b(o_t) = 1$ and $s_t = s$} \\
	0\ \textrm{ otherwise}
	\end{cases}
$$

Feature-state pairs provide useful additional information about which features and states go together and which don’t. For example, we can expect that pairs like "is_capitalized" + “B-ORG” would occur more frequently than "is_capitalized" + “O”, capturing the fact that in English named entities are often capitalized.

### State Transition Distributions

State transition distributions have exponential form and contain a weighted sum of all feature-state pairs:

$$
P_{s\prime}(s|o) = \frac{1}{Z(o, s\prime)} \exp\left(\sum_{a} \lambda_a \ f_a(o, s)\right)
$$

where $s\prime$ and $s$ are the previous and current state, $o$ is the current observation, $a$ is a feature-state pair, $\lambda_a$ is the learned weight for $a$, and $Z(o, s\prime)$ is a normalizing term to make the distribution $P_{s\prime}$ sum to one across all next states $s$.

The distribution's exponential form is a direct result of the maximum entropy principle that underlies MEMMs’ statistical structure and gives them their name. The principle states that the model that best represents a given system is one that makes the fewest possible assumptions except for certain constraints derived from training data. [@McCallum2000; @Pietra1997]

Here's a breakdown of the probability calculation:

<MEMMFeatureBreakdown />

### Training & Inference

The training step involves learning the weights $\lambda_a$ that satisfy MEMMs’ maximum entropy constraint. Learning is done through Generalized Iterative Scaling, which iteratively updates the values $\lambda_a$ in order to nudge the expected value of all features closer to their train set average [@McCallum2000]. Convergence at a global optimum is guaranteed given the exponential form of the transition distribution.

Inference is done using the Viterbi algorithm, like with HMMs [@McCallum2000; @Viterbi1967]. Rather than multiplying the previous probability with a transition and emission probability, here there is only a transition probability as described above.

### Results

A MEMM was trained with the CoNLL-2003 English dataset [@Sang2003]. Outside of word identity, additional features used for training include a lowercase version (“Algeria” → “algeria”), word shape, letter case (title case and uppercase), and whether the word contained only digits.

Here's the model in action:

<LivePrediction
	models={['memm']}
	initialInputValue="Chicago Board of Trade"
	label="Name tag predictions by MEMM:"
	mb={3}
/>

Overall, the model has better performance than its HMM equivalent. Per-word accuracy is slightly higher than the HMM’s 90.1%:

<MEMMPerWordScorecard />

Per-entity precision and recall are also notably higher (up from 64.2% and 55.8%, respectively):

<MEMMPerEntityScorecards />

A large part of the performance boost is attributable to better performance on OOV words:

<BinaryOOVHeatmap models={['memm', 'hmm']} />

### Advantage Over HMMs

As mentioned, the biggest advantage that MEMMs have over HMMs is their ability to model word features. This is important because with out-of-vocabulary words, which the CoNLL-2003 test set has plenty of, word identity alone contains no useful information – the model has never seen that word before. In those cases, derived features like word shape and capitalization are compelling stand-ins for word identity, helping the model make educated guesses about the hidden name tag.

<EntityLengthHeatmap models={['memm', 'hmm']} />

<OOVRateHeatmap models={['memm', 'hmm']} />

---

<References />
