import { Fragment } from 'react'

import LivePrediction from './livePrediction'
import ABCDGraph from './partI/sampling/abcdGraph'
import StaticABCDGraph from './partI/staticABCDGraph'
import HMMEmissionGraph from './partII/emissionGraph'
import HMMEmissionPaths from './partII/emissionPaths'
import HMMNERGraph from './partII/nerGraph'
import HMMOverviewGraph from './partII/overviewGraph'
import HMMPerEntityScorecards from './partII/perEntityScorecards'
import HMMPerWordScorecard from './partII/perWordScorecard'
import HMMTransitionGraph from './partII/transitionGraph'
import HMMTransitionPaths from './partII/transitionPaths'
import MEMMDiscriminativeGraph from './partIII/discriminativeGraph'
import MEMMFeatureBreakdown from './partIII/featureBreakdown'
import MEMMOverviewGraph from './partIII/overviewGraph'
import MEMMPerEntityScorecards from './partIII/perEntityScorecards'
import MEMMPerWordScorecard from './partIII/perWordScorecards'
import BinaryOOVHeatmap from './precisionRecallHeatmaps/binaryOOVHeatmap'
import EntityLengthHeatmap from './precisionRecallHeatmaps/entityLengthHeatmap'
import OOVRateHeatmap from './precisionRecallHeatmaps/oovRateHeatmap'

<Header>

# Learning What’s in a Name with Graphical Models

{

<Abstract>
	An overview of the structure and statistical assumptions behind linear-chain graphical
	models – effective and interpretable approaches to sequence labeling problems such as
	named entity recognition.
</Abstract>
}

<LivePrediction
	hideTagPrefixes
	models={['hmm', 'memm', 'crf']}
	initialInputValue="East Timorese-born activist Jose Ramos Horta"
	label={
		<Fragment>
			Which of these words refer to a&nbsp;
			<abbr title="A person, organization, location, or other (miscellaneous)">
				<span>named entity</span>
			</abbr>?
		</Fragment>
	}
/>

</Header>

---

## III. Maximum Entropy Markov Models

Maximum Entropy Markov Models (MEMMs) resolve some of the concerns with HMMs by way of a simple yet meaningful modification in graphical structure:

<MEMMOverviewGraph />

Similar to HMMs, MEMMs have been successfully applied to a wide range of sequence modeling problems [@McCallum2000; @Thierry2010; @Ziebart2009].

### Discriminative Structure

Suppose there is a system with two variables, $X$ and $Y$, and we want to make predictions about $Y$ based on observations of $X$. A generative classifier would do that by first learning the prior distribution $p(X, Y)$ and then applying Bayes' rule to find the posterior $p(Y|X)$. In a way, it attempts to reconstruct the process that generated the observed data. A discriminative classifier, on the other hand, would model the posterior $p(Y|X)$ directly based on training data. [@Ng2001]

While HMMs are generative classifiers, MEMMs are discriminative. The former are generative because they model the joint distribution over both observations and hidden states (as a product of transition and emission probabilities) before using that joint distribution to find the most likely state sequence given observations (or solve some other inference problem). MEMMs, on the other hand, directly model the conditional distribution $p(states|observations)$ without any intermediaries.

<MEMMDiscriminativeGraph />

### Word Features

The biggest advatage that MEMMs have over HMMs is their ability to model overlapping word features. Two features can overlap when they contain the same or similar pieces of information, like word shape (“Xxx”) and capitalization. HMMs don’t allow overlapping features, since as a result of their generative structure they require that all events in the observation layer be independent of one another. MEMMs, on the other hand, are discriminative and able to relax the independence requirement, so they can use any number of word features, overlapping or not. [@McCallum2000]

Common practice is to use binary features, such as:

$$
b(o_t) = \begin{cases}
	1\ \textrm{ if $o_t$ has shape "Xxx"} \\
	0\ \textrm{ otherwise}
	\end{cases}
$$

These features are then paired with the current state $s$ to form feature-state pairs $a = \langle b, s \rangle$:

$$
f_{\langle b, s \rangle}(o_t, s_t) = \begin{cases}
	1\ \textrm{ if $b(o_t) = 1$ and $s_t = s$} \\
	0\ \textrm{ otherwise}
	\end{cases}
$$

Feature-state pairs provide useful additional information about which features and states go together and which don’t. For example, we can expect that pairs like "is_capitalized" + “B-ORG” would occur more frequently than "is_capitalized" + “O”, capturing the fact that in English named entities are often capitalized.

### State Transitions

State transition distributions have exponential form and contain a weighted sum of all feature-state pairs:

$$
P_{s\prime}(s|o) = \frac{1}{Z(o, s\prime)} \exp\left(\sum_{a} \lambda_a \ f_a(o, s)\right)
$$

where $s\prime$ and $s$ are the previous and current state, $o$ is the current observation, $a$ is a feature-state pair, $\lambda_a$ is the learned weight for $a$, and $Z(o, s\prime)$ is a normalizing term to make the distribution $P_{s\prime}$ sum to one across all next states $s$.

Those familiar with neural networks will recognize that the distribution function is a softmax. Its exponential form results from a core principle that underlies MEMMs’ statistical structure: that of maximum entropy, which states that the model that best represents our knowledge about a system is one that makes the fewest possible assumptions except for certain constraints derived from prior data from that system. [@McCallum2000; @Pietra1997]

<MEMMFeatureBreakdown />

### Training & Inference

The training step involves learning the weights $\lambda_a$ that satisfy MEMMs’ maximum entropy constraint. Learning is done through Generalized Iterative Scaling, which iteratively updates the values $\lambda_a$ in order to nudge the expected value of all features closer to their train set average [@McCallum2000]. Convergence at a global optimum is guaranteed given the exponential form of the transition distribution.

As with HMMs, inference is done using the Viterbi algorithm [@McCallum2000; @Viterbi1967]. Rather than multiplying the previous probability with a transition and emission probability, here there is only a transition probability as described above.

### Results

A MEMM was trained on the CoNLL-2003 English dataset [@Sang2003]. In addition to word identity, features used for training include the word’s lowercase version (“Algeria” → “algeria”), shape (“Xxxx”), whether it is in title/upper case, and whether it contains only digits and no letter.

Here’s a live version of the trained model:

<LivePrediction
	models={['memm']}
	initialInputValue="Chicago Board of Trade"
	label="Name tag predictions by MEMM:"
	mb={3}
/>

The model has better performance than its HMM counterpart. Per-word accuracy is higher than the HMM’s 90.1%:

<MEMMPerWordScorecard />

Per-entity precision and recall are notably higher, up from 64.2% and 55.8%, respectively:

<MEMMPerEntityScorecards />

A large part of the performance boost is attributable to better performance on entities with OOV words:

<BinaryOOVHeatmap models={['memm', 'hmm']} />

This likely has to do with MEMM’s ability to model word features and deserves further discussion.

### Advantage Over HMMs

As mentioned, the biggest advantage that MEMMs have over HMMs is their ability to model word features. With out-of-vocabulary words, which the CoNLL-2003 test set has plenty of, word identity alone provides no useful information – the model has never seen the word before. In those cases, derived features such as word shape and capitalization can function as imperfect but immensely helpful proxies for word identity, allowing the model to make better guesses at the hidden name tag. Precision and recall scores for OOV-dense entities are compelling evidence of this effect:

<OOVRateHeatmap models={['memm', 'hmm']} />

With higher predictive power on OOV words, we can additionally expect better performance on long, multi-word entities. That’s because OOV words are dangerous information gaps inside named entities. They’re easy to misclassify, and when they are the entire predicted entity is counted as incorrect. MEMMs are able to fill those gaps to an extent using derived word features. As a result, we don’t see as drastic of a performance deterioration for longer entities as observed with the HMM:

<EntityLengthHeatmap models={['memm', 'hmm']} />

---

<References />
