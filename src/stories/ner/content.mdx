import { Fragment } from 'react'

import LivePrediction from './livePrediction'
import ABCDGraph from './partI/sampling/abcdGraph'
import StaticABCDGraph from './partI/staticABCDGraph'
import EmissionGraph from './partII/emissionGraph'
import EmissionPaths from './partII/emissionPaths'
import EntityLengthHeatmap from './partII/entityLengthHeatmap'
import NERGraph from './partII/nerGraph'
import OverviewGraph from './partII/overviewGraph'
import PerEntityScorecards from './partII/perEntityScorecards'
import PerWordScorecards from './partII/perWordScorecards'
import TransitionGraph from './partII/transitionGraph'
import TransitionPaths from './partII/transitionPaths'

<Header>

# Learning What’s in a Name with Graphical Models

{

<Abstract>
	An overview of the structure and statistical assumptions behind linear-chain graphical
	models – effective and interpretable approaches to sequence labeling problems such as
	named entity recognition.
</Abstract>
}

<LivePrediction
	hideTagPrefixes
	models={['hmm', 'crf']}
	initialInputValue="The UK Department of Transport"
	label={
		<Fragment>
			Which of these words refer to a&nbsp;
			<abbr title="A person, organization, location, or other (miscellaneous)">
				<span>named entity</span>
			</abbr>?
		</Fragment>
	}
/>

</Header>

“The UK” alone is a country, but “The UK Department of Transport” is an organization within said country. In a named entity recognition (NER) task, where we want to label each word with a name tag (organization/person/location/other/not a name) [@Chinchor1998], how can a computer model know one from the other?

In such cases, contextual information is key. In the second example, the fact that “The UK” is followed by “Department” is compelling evidence that when taken together the phrase refers to an organization. Sequence models – machine learning systems designed to take sequences of data as input, can recognize and put such relationships to productive use. Rather than making isolated predictions based on individual words or word groups, they take the given sequence as a combined unit, model the dependencies between words in that sequences, and depending on the problem can return the most likely label sequence.

In this article, we’ll explore three sequence models that are particularly successful at NER: Hidden Markov Models (HMMs), Maximum-Entropy Markov Models (MEMMs), and Conditional Random Fields (CRFs). All three are probabilistic graphical models, which we’ll cover in the next section.

---

## I. Probabilistic Graphical Models

Graphical modeling is a robust framework for representing probabilistic models. Complex multivariate probability distributions can be expressed with compact graphs that are vastly easier to understand and interpret.

### Factorizing Joint Distributions

Let’s start with a simple example with just two random variables, $A$ and $B$. Assume that $B$ is conditionally dependent on $A$. Through a canonical application of the chain rule, the joint distribution of $A$ and $B$ is:

$$
p(a, b) = p(a) \cdot p(b|a)
$$

{

<Alert title="Notation:" marginBottom={3}>
	the shorthand <em>p(a)</em> means <em>p(A = a)</em>, that is, the probability of
	variable A taking value a.
</Alert>
}

This is a simple enough example, with just 2 factors in the right hand side. Add more variables, however, and the result can get messy fast. To see this, assume that there are two more variables, $C$ and $D$, and that $D$ is conditionally dependent on $A$, $B$, and $C$. The factorization becomes:

$$
p(a, b, c, d) = p(a) \cdot p(b|a) \cdot p(c) \cdot p(d|a, b, c)
$$

The relationship between variables is more opaque, hidden behind second-order dependencies. For example, while it’s clear that $D$ is directly dependent on $A$, we may miss the fact that there is another, second-order dependency between the two ($D$ is dependent on $B$, which in turn is dependent on $A$).

### Directed Acyclic Graphs

Directed Acyclic Graphs, or DAGs, offer a natural remedy to this problem. Each factor in the equation can be represented by a node. An arrow indicates conditional dependence. The resulting graph would look like:

<StaticABCDGraph />

With this graph, it’s easier to construct a generative story of how $A$, $B$, $C$ and $D$ are sampled. The process proceeds in [topological order](https://en.wikipedia.org/wiki/Topological_sorting), for example $A$ → $C$ → $B$ → $D$, to ensure that all dependencies have been resolved by the time each variable is sampled.

Below is what a sampled population of the given distributions would look like. For the sake of demonstration, some distribution parameters are modifiable – in reality these are the quantities that need to be learned from training data.

<ABCDGraph />

For more detailed accounts of probabilistic graphical models, consider reading the textbooks _Probabilistic Graphical Models: Principles and Techniques_ by Daphne Koller and Nir Friedman [@Koller2009] and _Probabilistic Reasoning in Intelligent Systems_ by Judea Pearl [@Pearl1988].

---

## II. Hidden Markov Models

Hidden Markov Models (HMMs) are an early class of probabilistic graphical models representing partially hidden (unobserved) sequences of events. Structurally, they are built with two main layers, one hidden ($S_i$) and one observed ($O_i$):

<OverviewGraph />

HMMs have been successfully applied to a wide range of problems, including gene analysis [@Shatkay2000], information extraction [@Leek1997; @Freitag1999], speech recognition [@Rabiner1989], as well ass named entity recognition [@Bikel1999].

### The Hidden Layer

The hidden layer is assumed to be a Markov process: a chain of events in which each event’s probability depends only on the state of the preceding event. More formally, given a sequence of $N$ random events $S_1$, $S_2$,…, $S_N$, the Markov assumption holds that:

$$
p(s_i | s_1, s_2,…, s_{i-1}) = p(s_i | s_{i-1}) \\
\footnotesize \textrm{for all $i \in \{2,…, N\}$}
$$

In a graph, this translates to a linear chain of events where each event has exactly one arrow pointing towards it (except for the first event) and one pointing away from it (except for the last):

<TransitionGraph />

A second assumption that HMMs make is time-homogeneity: that the probability of transition from one event's state to the next is constant over time. In formal terms:

$$
p(s_i | s_{i-1}) = p(s_{i+1} | s_i) \\
\footnotesize \textrm{for all $i \in \{2,…, N-1\}$}
$$

$p(s_i | s_{i-1})$ is called the transition probability and is one of the two key parameters to be learned during training.

The assumptions about the hidden layer – Markov and time-homogeneity – hold up in various time-based systems where the hidden, unobserved events occur sequentially, one after the other. Together, they meaningfully reduce the computational complexity of both learning and inference.

### The Observed Layer

The hidden and observed layer are connected via a one-to-one mapping relationship. The probability of each observed event is assumed to depend only on the state of the hidden event at the same time step. Given a sequence of $N$ hidden events $S_1$, $S_2$,…, $S_N$ and observed events $O_1$, $O_2$,…, $O_N$ we have:

$$
p(o_i | s_1, s_2,…, s_N) = p(o_i | s_i) \\
\footnotesize \textrm{for all $i \in \{1, 2,…, N\}$}
$$

In a graph, this one-to-one relationship looks like:

<EmissionGraph />

The conditional probability $p(o_i | s_i)$, called the emission probability, is also assumed to be time-homogenous, further reducing the model's complexity. It is the second key parameter to be learned, alongside the transition probability.

### Representing Named Entities

HMMs’ chain structure is particularly useful in sequence labeling problems like NER. For each input text sequence, the observed layer represents known word tokens, while the hidden layer contains their respective name tags:

<NERGraph />

{

<Alert title="Representation:" marginBottom={3}>
	rather than labeling each node using the name of the variable it represents (X&#8321;,
	Y&#8321;) as we have until this point, we'll instead display the value of that variable
	(“O”, “Great”). This helps make the graphs easier to read.
</Alert>
}

There are 9 possible name tags. Each, apart from the “O” tag, has either a B- (beginning) or I- (inside) prefix, to eliminate confusion about when an entity stops and the next one begins.

Between any two consecutive hidden states, there are 9&#178; = 81 possible transitions. Each transition has its own probability, $p(x_i|x_{i-1})$:

<TransitionPaths
	nStates={2}
	label="Higher opacity indicates higher relative probability."
/>

In the observed layer, each node can have any value from the vocabulary, whose size ranges anywhere from the thousands to the hundreds of thousands. The vocabulary created for the HMM in this article contains 23,622 tokens. Let N be the number of tokens in the vocabulary. The number of possible emission probabilities is 9N ($n_{states} \cdot n_{tokens}$).

<EmissionPaths label="Higher opacity indicates higher relative probability." />

### Training

There are three sets of parameters to be learned during training: the transition, emission, and start probabilities. All can be computed as normalized rates of occurrence from the training data.

For example, to get the transition probability from state “O” to state “B-LOC”, $p(B{-}LOC | O)$, we need only two numbers: the number of times state “O” is followed by any other state (that is, it isn't the last state in the sequence), as $N_O$, and the number of times state “O” is followed by state “B-LOC”, as $N_{O→B-LOC}$. The desired transition probaility is simply $\frac{N_{O→B-LOC}}{N_O}$. The same calculation can be done for each of the remaining probabilities.

### Inference

In the context of HMMs, inference involves answering useful questions about hidden states given observed values, or about missing values given a partially observed sequence. In NER, we are focused on the first type of inference. Specifically, we want to perform maximum a posteriori (MAP) inference to identify the most likely state sequence conditioned on observed values.

There is usually an intractably large number of candidate state sequences. For any two consecutive states, there are 81 potential transition paths. For three states there are 82&#178; paths. This number continues to grow exponentially as the number of states increases.

<TransitionPaths nStates={4} />

Luckily, there is an efficient dynamic algorithm that returns the most likely path with relatively low computational complexity: the Viterbi algorithm [@Viterbi1967]. It moves through the input sequence from left to right, at each step indentifying and saving the most likely path in a trellis-shaped memory structure. For more details, refer to the excellent description of the Viterbi algorithm in the book _Speech and Language Processing_ by Jurafsky & Martin [@Jurafsky2021].

### Results

An HMM with the structure outlined above was trained on the CoNLL-2003 English dataset [@Sang2003]. The train set contains 14,987 sentences and a total of 203,621 word tokens. Here's the model in action:

<LivePrediction
	models={['hmm']}
	label="Name tag predictions by HMM:"
	initialInputValue="East Timorese-born activist Jose Ramos Horta"
	mb={3}
/>

Evaluated against a test set, the model achieves satisfactory per-word accuracy:

<PerWordScorecards />

However, precision and recall – calculated per entity [@Sang2003] – are relatively low:

<PerEntityScorecards />

These metrics are lower than the per-word accuracy bacause they are entity-level evaluations that only count exact matches as true positives. Long, multi-word entities are considered incorrect if one or more of their constituent words are misidentified, in effect ignoring the other correctly identified tokens.

---

<EntityLengthHeatmap />

---

<References />
