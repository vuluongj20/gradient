import Demo from './demo'

<Header>

# Learning What’s in a Name with Graphical Models

{

<Abstract>
  An overview of the structure and statistical assumptions behind linear-chain graphical
  models – effective and interpretable approaches to sequence labeling problems such as
  named entity recognition.
</Abstract>
}

<Demo />

</Header>

“The UK” alone is a country, but “The UK Department of Transport” is an organization within said country. In a named entity recognition (NER) task, where we want to label each word with a name tag (organization/person/location/not a name), how can a computer model know one from the other?

In such cases, contextual information is key. In the second example, the fact that “The UK” is followed by “Department” is compelling evidence that when taken together the phrase refers to an organization. Sequence models – machine learning systems designed to take sequences of data as input, can recognize and put such relationships to productive use. Rather than making isolated predictions based on individual words, they take the given sequence as a combined unit, model the dependencies between words in that sequences, and depending on the problem can return the most likely label sequence.

In this article, we’ll explore three sequence models that are particularly successful at NER: Hidden Markov Models (HMMs), Maximum-Entropy Markov Models (MEMMs), and Conditional Random Fields (CRFs). All three are probabilistic graphical models, which we’ll cover in the next section.

---
