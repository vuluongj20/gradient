import Demo from './demo'

<Header>

# Learning What’s in a Name with Graphical Models

<Abstract>
  Linear-chain graphical models are compact, powerful and interpretable frameworks for
  studying natural language. In this article, we'll compare and contrast three classes of
  graphical models in the context of named entity recognition.
</Abstract>

<Demo />

</Header>

This chain of reasoning happens quickly and comes as second nature to human readers. But can we teach a machine to reason in a similar way? Can a language model disambiguate the role and meaning of words that are highly dependent on those around them?

Without resorting to complex neural networks, there are compact probabilistic models provide reliable and interpretable predictions in many sequence modeling problems, like part-of-speech tagging and named entity recognition.

In this article, we’ll explore three such models, each building on top of the one before: Hidden Markov Models (HMMs), Maximum-Entropy Markov Models (MEMMs), and Conditional Random Fields (CRFs). All three are probabilistic graphical models, which we’ll cover in the next section.

---
